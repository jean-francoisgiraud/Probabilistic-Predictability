# Probabilistic Predictability and Machine Learning

Knowing the degree of predictability and confidence in a process is a requirement in many fields such as sciences, engineering, technologies, medecine and justice.  

Current model: Machine learning mathematics> Probability> Frequentists> Basic notion of probability: # (number of) Results / # (number of) Attempts.   

Future model: Machine learning mathematics> Probability> Bayesian> Basic notion of probability: The probability is not a number, but a distribution itself.  


Parametric Data provides confidence levels in judgements and predictions.  

Is your data parametric? does it have probabilistic predictability?  
Can you prove it? If so how? (with the 4 statistical tests listed herein)  

IF the following four statistical tests (assumptions) are true  

1- a random sample is taken from a population with  
2- a fixed location (mean) with  
3- a fixed distribution (a relatively constant type of distribution such as a the normal distribution (bell curve)) and  
4- with a fixed variation (a relatively constant standard deviation)

THEN the data is parametric (it has a known distribution) AND probabilistic predictability is achieved, the process is in statistical control, repeatable and can be modeled (Yi=C+Ei=average+error) to make scientifically/legally valid predictions and conclusions (based on probability), for example: data will be Y +/- error 19 out of 20 times.  Control limits can be established and outliers determined/rejected, otherwise all data is significant, non-conforming results cannot be rejected arbitrarily, they must be rejected on the basis of data in a statistically controlled process.

ELSE the process is unpredictable, out of control, drifting and no conclusions-judgements for the past or predictions in the future can be made. Repeating the tests will yield different and unrelated results because there could be other unknown and unknowable variables not accounted for.

An example of unpredictable out of control data that cannot be used to make predictions is the 2016 United States elections. All media and pundits were completely wrong in predicting the results because their statistical models failed the 4 tests (the samples were not random with a constant mean and a constant distribution and constant variation).  

# The four tests are:  

Random sample (test 1) assumption  
The randomness assumption is true if Lag plot (data(i) vs data(i-1)) is structureless and random.  

Fixed mean (location) (test 2) assumption  
The fixed location assumption is true if the run sequence plot [trend line = mean] is flat and non-drifting.

Fixed normal distribution (test 3) assumption  
The fixed normal distribution is true if Histogram (frequency vs bins) is bell shaped (other shapes may indicate other type of distribution.  
The fixed distribution assumption is true if Normal probability plot (ordered Y versus theoretical ordered Y) is linear, a straight line means the data is normally distributed.  

Fixed variation (test 4) assumption  
The fixed variation assumption is true if in the Run sequence (data vs (i)or(id(test#))) the vertical spread is the same over entire x axis (there is minimal bends in the trend line)).   


IF the four assumptions hold  
THEN we have achieved probabilistic predictability (the ability to make probability statements about the process in the past and about the process in the future). Such processes are said to be "in statistical control".  
If the four assumptions are valid, then the process can be modeled and used to generate valid scientific and engineering conclusions.  
If the four assumptions are not valid, then the process is drifting (with respect to random-sampling, location=average, variation=StandardDeviation, or distribution), unpredictable, and out of control. A simple characterization of such processes by a location estimate, a variation estimate, or a distribution estimate inevitably leads to conclusions that are not valid, are not supportable (scientifically or legally), and which are not repeatable in the laboratory (these types of invalid claims are often made by people who do not understand statistics and rely instead on personal opinions or stereotyping (what seems or 'feels' right)) (Note-1).  
Each of the four assumptions must be routinely tested to verify the continuous validity of the recommendations and conclusions.
Extremes in data do not necessarily mean that they are due to a special cause (especially if the process is not in statistical control). They can still be part of the normal variation of the process.  

NOTES  
Note-1: Stereotyping is a type of heuristic that people use instead of data and statistical analysis to form opinions or make judgments about things they have not completely seen or experienced. They work as a mental shortcut to assess everything from the social status of a person based on their actions to assumptions that a plant that is tall, has a trunk, and has leaves is a tree even though the person making the evaluation has never seen that particular type of tree before. Stereotypes are the pictures people have in their heads that are built around experiences as well as what we are told about the world.  

REFERENCES  
Deming TQM, Toyota Production System, ESH (Engineering Statistics Handbook(1)), AS9100, ISO9001, Wikipedia (statistics and probabilities).  
(1). SEH Statistics Engineering Handbook https://www.itl.nist.gov/div898/handbook/
